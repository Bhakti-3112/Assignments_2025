{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "img=cv2.imread(\"image.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"grayimage\",gray)\n",
    "gaussian=cv2.GaussianBlur(img,(7,7),0)\n",
    "cv2.imshow(\"gaussian blur\",gaussian)\n",
    "cannyedges=cv2.Canny(image= img, threshold1=50, threshold2=150)\n",
    "cv2.imshow(\"cannyedges\",cannyedges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=cv2.imread(\"image.png\")\n",
    "\n",
    "width=img.shape[1]\n",
    "height=img.shape[0]\n",
    "imgsize=(width,height)\n",
    "center=(width//2,height//2)\n",
    "\n",
    "angle=60\n",
    "scale=1\n",
    "rotation_matrix=cv2.getRotationMatrix2D(center,angle,scale)\n",
    "rotated_image=cv2.warpAffine(img,rotation_matrix,imgsize)\n",
    "cv2.imshow(\"rotated_image\",rotated_image)\n",
    "\n",
    "scaling_factor=2\n",
    "new_width=int(width/scaling_factor)\n",
    "new_height=int(height/scaling_factor)\n",
    "new_imgsize=(new_width,new_height)\n",
    "scaled_image=cv2.resize(src= img, dsize=new_imgsize, interpolation=cv2.INTER_CUBIC)\n",
    "cv2.imshow(\"scaled_image\",scaled_image)\n",
    "\n",
    "flipby=1\n",
    "flipped_img=cv2.flip(img,flipby)\n",
    "cv2.imshow(\"flipped_image\",flipped_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "#splitting the dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "#designing the CNN\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "#training the model\n",
    "model.fit(X_train, y_train, epochs=7, batch_size=64)\n",
    "\n",
    "#evaluating the model\n",
    "model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv3 model \n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Load class names\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get output layer names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "def detect_objects(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "            color = (0, 255, 0)\n",
    "            \n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Detected Objects\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "img_paths = [\"room.jpg\", \"forest.jpg\", \"kitchen.jpg\"]\n",
    "\n",
    "for path in img_paths:\n",
    "    detect_objects(path)\n",
    "\n",
    "'''\n",
    "these models are used in object detection. they are used in autonomous detection for pedestrian identification,lane identification and obstacle\n",
    "detection. the image passes through the yolo model and the output consists of prediction from the 3 detection layers. along with this, there are\n",
    "two more features:- Confidence score and Class probability. confidence score indicates how sure it is that object is present inside the box and\n",
    "class probability indicates that the detected object belongs to a particular class\n",
    "'''  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"image.png\", 1)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray_blur=cv2.blur(gray_img,(3,3))\n",
    "circles=cv2.HoughCircles(gray_blur,cv2.HOUGH_GRADIENT,1,20,param1=50,param2=30,minRadius=1,maxRadius=40)\n",
    "circles=np.uint16(np.around(circles))\n",
    "for pt in circles[0,:]:\n",
    "    a,b,r=pt[0],pt[1],pt[2]\n",
    "    cv2.circle(img,(a,b),r,(0,255,0),2)\n",
    "\n",
    "triangles=[]\n",
    "edged=cv2.Canny(gray_img,50,150)\n",
    "contours,heirarchy=cv2.findContours(edged,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "for contour in contours:\n",
    "    epsilon=0.04*cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,epsilon,True)\n",
    "    if len(approx)==3:\n",
    "        triangles.append(approx)\n",
    "        cv2.drawContours(img,[approx],0,(255,0,0),2)\n",
    "\n",
    "for triangle in triangles:\n",
    "    for circle in circles[0,:]:\n",
    "        a,b,r=circle[0],circle[1],circle[2]\n",
    "        inside=True\n",
    "        for point in triangle:\n",
    "            px,py=point[0]\n",
    "            distance=np.sqrt((px-a)**2 + (py-b)**2)\n",
    "            if distance>r:\n",
    "                inside=False\n",
    "                break\n",
    "        if inside:\n",
    "            cv2.drawContours(img,[triangle],0,(0,0,255),3)\n",
    "            cv2.circle(img,(a,b),r,(0,255,255),3)\n",
    "\n",
    "cv2.imshow(\"final image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
