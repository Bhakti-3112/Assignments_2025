{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ðŸ”¹ Step 1: Training on a Pre-built Dataset (Roboflow)\n",
    "I began this project by using a pre-made dataset obtained from Roboflow, a web-based tool that offers datasets with annotated images and configuration files. The dataset provided both the images and YOLO-formatted annotations, along with a ready-to-use config.yaml file. This helped me quickly get started with model training.\n",
    "\n",
    "I trained a YOLOv8 model on this dataset using the Ultralytics library. After training, I evaluated the model by calculating the following key metrics:\n",
    "\n",
    "mAP@0.5\n",
    "\n",
    "mAP@0.5:0.95\n",
    "\n",
    "Finally, I exported the trained model into ONNX format, which allows the model to be used across different platforms and tools, such as TensorRT and OpenCV.\n",
    "\n",
    "ðŸ”¹ Step 2: Creating and Training on a Custom Dataset\n",
    "For the second part of the project, I created my own custom dataset:\n",
    "\n",
    "I collected approximately 120 images from Google and other online sources.\n",
    "\n",
    "I used CVAT (Computer Vision Annotation Tool) to annotate the images with bounding boxes.\n",
    "\n",
    "After annotation, I saved the images and their corresponding YOLO-format label files in the appropriate folder structure (images/train, labels/train).\n",
    "\n",
    "I then trained a YOLOv8 model on this custom dataset and once again calculated the performance metrics (mAP@0.5, mAP@0.5:0.95) to evaluate the results. After confirming the training was successful, I exported the model into ONNX format for deployment purposes.\n",
    "\n",
    "ðŸ”¹ Additional Learning & Tools Used\n",
    "To complete this project, I watched YouTube tutorials to understand:\n",
    "\n",
    "How to train YOLOv8 on a custom dataset\n",
    "\n",
    "The concepts of mAP (mean Average Precision) and IoU (Intersection over Union)\n",
    "\n",
    "How to export models to ONNX\n",
    "\n",
    "Through this process, I gained hands-on experience in:\n",
    "\n",
    "Dataset preparation and annotation\n",
    "\n",
    "Object detection training workflows\n",
    "\n",
    "Model evaluation using mAP and IoU\n",
    "\n",
    "Exporting models for cross-platform inference using ONNX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object detection using Roboflow Dataset \n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")\n",
    "\n",
    "# Training the model\n",
    "results = model.train(data=r\"C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\data.yaml\", epochs=3)\n",
    "                      \n",
    "metrics = model.val(data=r\"C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\data.yaml\")                    \n",
    "\n",
    "# Print the mAP scores\n",
    "print(\"mAP@0.5:\", metrics.box.map50)          # mean Average Precision at IoU 0.5\n",
    "print(\"mAP@0.5:0.95:\", metrics.box.map)       # mean AP averaged over IoU from 0.5 to 0.95\n",
    "\n",
    "model.export(format=\"onnx\", dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa030b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output:- \n",
    "\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "engine\\trainer: task=detect, mode=train, model=yolov8n.yaml, data=C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\data.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
    "Overriding model.yaml nc=80 with nc=1\n",
    "\n",
    "                   from  n    params  module                                       arguments                     \n",
    "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
    "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
    "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
    "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
    "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
    "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
    "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
    "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
    "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
    "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
    " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
    " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
    " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
    " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
    " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
    " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
    " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
    "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
    "\n",
    "TensorBoard: Start with 'tensorboard --logdir runs\\detect\\train3', view at http://localhost:6006/\n",
    "Freezing layer 'model.22.dfl.conv.weight'\n",
    "train: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\train\\labels.cache... 644 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 644/644 [00:00<?, ?it/s]\n",
    "WARNING  Box and segment counts should be equal, but got len(segments) = 114, len(boxes) = 682. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
    "\n",
    "val: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\valid\\labels.cache... 181 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<?, ?it/s]\n",
    "WARNING  Box and segment counts should be equal, but got len(segments) = 36, len(boxes) = 185. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
    "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
    "\n",
    "optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
    "optimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
    "TensorBoard: model graph visualization added \n",
    "Image sizes 640 train, 640 val\n",
    "Using 0 dataloader workers\n",
    "Logging results to runs\\detect\\train3\n",
    "Starting training for 3 epochs...\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        1/3         0G      3.057      3.776      4.235         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:26<00:00,  3.58s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:15<00:00,  2.51s/it]\n",
    "                   all        181        185    0.00289      0.849    0.00593    0.00163\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        2/3         0G      2.971      3.466      3.901          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:23<00:00,  3.51s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.36s/it]\n",
    "                   all        181        185    0.00289      0.849     0.0206    0.00408\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "        3/3         0G      2.963      3.191      3.637          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [05:14<00:00,  7.67s/it]  \n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.45s/it]\n",
    "                   all        181        185      0.603     0.0411      0.148     0.0515\n",
    "\n",
    "3 epochs completed in 0.181 hours.\n",
    "\n",
    "Optimizer stripped from runs\\detect\\train3\\weights\\last.pt, 6.2MB\n",
    "Optimizer stripped from runs\\detect\\train3\\weights\\best.pt, 6.2MB\n",
    "\n",
    "Validating runs\\detect\\train3\\weights\\best.pt...\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.09s/it]\n",
    "                   all        181        185      0.602      0.041      0.148     0.0515\n",
    "Speed: 1.3ms preprocess, 60.3ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
    "Results saved to runs\\detect\\train3\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "val: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\valid\\labels.cache... 181 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<?, ?it/s]\n",
    "WARNING  Box and segment counts should be equal, but got len(segments) = 36, len(boxes) = 185. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
    "\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]\n",
    "                   all        181        185      0.602      0.041      0.148     0.0515\n",
    "Speed: 1.2ms preprocess, 55.5ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
    "Results saved to runs\\detect\\train32\n",
    "mAP@0.5: 0.14801390741739\n",
    "mAP@0.5:0.95: 0.05153082957473977\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "\n",
    "PyTorch: starting from 'runs\\detect\\train3\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (6.0 MB)\n",
    "requirements: Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime'] not found, attempting AutoUpdate...\n",
    "Collecting onnx>=1.12.0\n",
    "  Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
    "Collecting onnxslim\n",
    "  Downloading onnxslim-0.1.50-py3-none-any.whl.metadata (4.8 kB)\n",
    "Collecting onnxruntime\n",
    "  Downloading onnxruntime-1.21.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
    "Requirement already satisfied: numpy>=1.20 in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from onnx>=1.12.0) (1.26.4)\n",
    "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from onnx>=1.12.0) (4.25.3)\n",
    "Requirement already satisfied: sympy in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from onnxslim) (1.13.1)\n",
    "Requirement already satisfied: packaging in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from onnxslim) (24.1)\n",
    "Collecting coloredlogs (from onnxruntime)\n",
    "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
    "Requirement already satisfied: flatbuffers in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
    "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
    "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
    "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gunem\\anaconda3\\lib\\site-packages (from sympy->onnxslim) (1.3.0)\n",
    "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
    "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
    "Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
    "   ---------------------------------------- 14.5/14.5 MB 11.9 MB/s eta 0:00:00\n",
    "Downloading onnxslim-0.1.50-py3-none-any.whl (144 kB)\n",
    "Downloading onnxruntime-1.21.0-cp312-cp312-win_amd64.whl (11.8 MB)\n",
    "   ---------------------------------------- 11.8/11.8 MB 11.9 MB/s eta 0:00:00\n",
    "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
    "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
    "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
    "Installing collected packages: pyreadline3, onnx, onnxslim, humanfriendly, coloredlogs, onnxruntime\n",
    "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.0 onnxslim-0.1.50 pyreadline3-3.5.4\n",
    "\n",
    "requirements: AutoUpdate success  20.8s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim', 'onnxruntime']\n",
    "requirements:  Restart runtime or rerun command for updates to take effect\n",
    "\n",
    "\n",
    "ONNX: starting export with onnx 1.17.0 opset 19...\n",
    "ONNX: slimming with onnxslim 0.1.50...\n",
    "ONNX: export success  32.3s, saved as 'runs\\detect\\train3\\weights\\best.onnx' (11.6 MB)\n",
    "\n",
    "Export complete (32.5s)\n",
    "Results saved to C:\\Users\\gunem\\runs\\detect\\train3\\weights\n",
    "Predict:         yolo predict task=detect model=runs\\detect\\train3\\weights\\best.onnx imgsz=640  \n",
    "Validate:        yolo val task=detect model=runs\\detect\\train3\\weights\\best.onnx imgsz=640 data=C:\\Users\\gunem\\OneDrive\\Desktop\\let's move\\rakshak work\\data\\data.yaml  \n",
    "Visualize:       https://netron.app\n",
    "'runs\\\\detect\\\\train3\\\\weights\\\\best.onnx'\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object detection using custom dataset\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "\n",
    "# Use the model\n",
    "results = model.train(data=r\"C:\\Users\\gunem\\OneDrive\\Desktop\\data\\config.yaml\", epochs=5)  # train the model\n",
    "\n",
    "metrics = model.val(data=r\"C:\\Users\\gunem\\OneDrive\\Desktop\\data\\config.yaml\")                    \n",
    "\n",
    "# Print the mAP scores\n",
    "print(\"mAP@0.5:\", metrics.box.map50)          # mean Average Precision at IoU 0.5\n",
    "print(\"mAP@0.5:0.95:\", metrics.box.map)       # mean AP averaged over IoU from 0.5 to 0.95\n",
    "\n",
    "model.export(format=\"onnx\", dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yaml file:-\n",
    "\n",
    "path: C:\\Users\\gunem\\OneDrive\\Desktop\\data\n",
    "\n",
    "train: C:\\Users\\gunem\\OneDrive\\Desktop\\data\\images\\train\n",
    "val: C:\\Users\\gunem\\OneDrive\\Desktop\\data\\images\\train\n",
    "\n",
    "nc: 1\n",
    "names: ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d54241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output:- \n",
    "\n",
    "New https://pypi.org/project/ultralytics/8.3.106 available  Update with 'pip install -U ultralytics'\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "engine\\trainer: task=detect, mode=train, model=yolov8n.yaml, data=C:\\Users\\gunem\\OneDrive\\Desktop\\data\\config.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train7\n",
    "Overriding model.yaml nc=80 with nc=1\n",
    "\n",
    "                   from  n    params  module                                       arguments                     \n",
    "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
    "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
    "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
    "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
    "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
    "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
    "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
    "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
    "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
    "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
    " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
    " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
    " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
    " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
    " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
    " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
    " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
    "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
    "\n",
    "TensorBoard: Start with 'tensorboard --logdir runs\\detect\\train7', view at http://localhost:6006/\n",
    "Freezing layer 'model.22.dfl.conv.weight'\n",
    "train: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\data\\labels\\train... 119 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<00:00, 620.21it/s]\n",
    "train: New cache created: C:\\Users\\gunem\\OneDrive\\Desktop\\data\\labels\\train.cache\n",
    "val: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\data\\labels\\train.cache... 119 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<?, ?it/s]\n",
    "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
    "optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
    "optimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
    "TensorBoard: model graph visualization added \n",
    "Image sizes 640 train, 640 val\n",
    "Using 0 dataloader workers\n",
    "Logging results to runs\\detect\\train7\n",
    "Starting training for 5 epochs...\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        1/5         0G      2.985      3.988       4.52         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:16<00:00,  9.57s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  6.85s/it]\n",
    "                   all        119        127    0.00303       0.85    0.00394    0.00134\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        2/5         0G      3.071      3.813       4.32         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:13<00:00,  9.22s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  6.63s/it]\n",
    "                   all        119        127    0.00303       0.85    0.00426    0.00131\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        3/5         0G      2.928       3.56      4.186         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:13<00:00,  9.17s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  6.58s/it]\n",
    "                   all        119        127    0.00305      0.858    0.00431    0.00139\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        4/5         0G      2.958      3.528       4.16         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:11<00:00,  8.95s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  6.89s/it]\n",
    "                   all        119        127    0.00305      0.858     0.0041    0.00144\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        5/5         0G      2.843      3.243      4.089         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:13<00:00,  9.21s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  6.65s/it]\n",
    "                   all        119        127    0.00305      0.858    0.00409    0.00148\n",
    "\n",
    "\n",
    "5 epochs completed in 0.144 hours.\n",
    "Optimizer stripped from runs\\detect\\train7\\weights\\last.pt, 6.2MB\n",
    "Optimizer stripped from runs\\detect\\train7\\weights\\best.pt, 6.2MB\n",
    "\n",
    "Validating runs\\detect\\train7\\weights\\best.pt...\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:24<00:00,  6.07s/it]\n",
    "                   all        119        127    0.00305      0.858    0.00406    0.00147\n",
    "Speed: 3.1ms preprocess, 179.2ms inference, 0.0ms loss, 7.0ms postprocess per image\n",
    "Results saved to runs\\detect\\train7\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "val: Scanning C:\\Users\\gunem\\OneDrive\\Desktop\\data\\labels\\train.cache... 119 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<?, ?it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:23<00:00,  2.88s/it]\n",
    "                   all        119        127    0.00305      0.858    0.00406    0.00147\n",
    "Speed: 3.5ms preprocess, 170.0ms inference, 0.0ms loss, 5.5ms postprocess per image\n",
    "Results saved to runs\\detect\\train72\n",
    "mAP@0.5: 0.004055573527865937\n",
    "mAP@0.5:0.95: 0.001467717992815073\n",
    "Ultralytics 8.3.105  Python-3.12.7 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
    "\n",
    "PyTorch: starting from 'runs\\detect\\train7\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (6.0 MB)\n",
    "\n",
    "ONNX: starting export with onnx 1.17.0 opset 19...\n",
    "ONNX: slimming with onnxslim 0.1.50...\n",
    "ONNX: export success  31.5s, saved as 'runs\\detect\\train7\\weights\\best.onnx' (11.6 MB)\n",
    "\n",
    "Export complete (32.3s)\n",
    "Results saved to C:\\Users\\gunem\\runs\\detect\\train7\\weights\n",
    "Predict:         yolo predict task=detect model=runs\\detect\\train7\\weights\\best.onnx imgsz=640  \n",
    "Validate:        yolo val task=detect model=runs\\detect\\train7\\weights\\best.onnx imgsz=640 data=C:\\Users\\gunem\\OneDrive\\Desktop\\data\\config.yaml  \n",
    "Visualize:       https://netron.app\n",
    "'runs\\\\detect\\\\train7\\\\weights\\\\best.onnx'\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
